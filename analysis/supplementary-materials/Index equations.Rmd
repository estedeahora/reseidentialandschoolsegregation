## Appendix A

**Dissimilarity index** ($D_{k_1,k_2}$)

La fórmula del Índice de Disimilitud para los grupos $k_1$ y $k_2$ es:

$$
  D_{k_1,k_2} = \frac{1}{2}.\sum_{i=1}^n\lvert{\frac{x_i^{k_1}}{X^{k_1}}-\frac{x_i^{k_2}}{X^{k_2}}}\rvert
$$

Donde $x_i^k$ es la cantidad de personas de la clase $k$ en la unidad $i$ y $X^k$ es la cantidad de total de personas del grupo $k$ en la población total.

Para el índice de disimilitud existe una adaptación espacial propuesta por [@Wong1993; un desarrollo de esta propuesta puede encontrarse en @Yao2019], que genera un ajuste por el ratio de área, el perímetro y el límite ($D_{k_1;k_2}(s)$). Al igual que el índice de disimilitud, permite comparar entre pares de grupos (es decir, no es un índice multigrupo). Si bien no presentamos el resultado de este índice para los radios, el mismo tiene valores muy similares al índice de disimilitud sin corrección espacial. Optamos por utilizar el índice de disimilitud sin la corrección espacial, para facilitar una comparación más exacta con la segregación escolar.

**Multi-group dissimilarity** ($D^*$)

La versión para múltiples grupos del índice de disimilitud es:

$$
  D^* = \frac{1}{2IS} \sum_{k=1}^{K}{\pi_k\sum_{i=1}^{I}{\frac{t_i}{T}|r_{ik}-1|}}
$$

Donde para las $I$ unidades espaciales cuyos individuos se encuentran distribuidos en $K$ categorías, $IS$ es el Indice de Interacción de @Simpson1949; $\pi_k$ es la proporción de unidades de la categoría $k$, $t_i$ es el número de individuos en la unidad $i$; $T$ es el número total de individuos; y $r_{ik}$ es el ratio $\frac{\pi_{ik}} {\pi_k}$.

***Mutual Information Index*** ($M$)

El Índice de Información Mutua es un índice no estandarizado, que basa su capacidad de descomposición es su simetría y su dependencia marginal en ambas direcciones. Estas propiedades pueden verse en su fórmula matemática:

$$
  M = \sum_{u}{p_{\cdot u}L_u} = \sum_{u} p_{\cdot u}(\sum_{g}p_{g|u} \log{\frac{p_{g|u}}{p_{g \cdot}}}) =
  $$

$$
  \sum_{g}{p_{g \cdot }L_g} = \sum_{g} p_{g \cdot}(\sum_{u}p_{u|g} \log{\frac{p_{u|g}}{p_{\cdot u}}})
$$

$$
  M = \sum_{u}{p_{\cdot u} L_u}=\sum_{g}{p_{g \cdot } L_g}
$$

Donde, dadas las unidades $u$ y los grupos $g$, se define $p_{\cdot u}$ y $p_{g\cdot}$ como la probabilidad marginal de cada una de las unidades/grupos y $p_{g|u} = \frac {P_{gu}}{p_{\cdot u}}$ como la probabilidad condicional de que ocurra $g$, dada la unidad $u$. El valor $L_u$ y $L_g$ representan el puntaje de segregación local de la unidad $u$ o el grupo $g$ según corresponda.

A su vez, el índice $M$ puede expresarse en función del índice de entropía $E(\cdot)=-\sum_i{p_i \log{p_i} }$, como:

$$
  M = \sum_u{p_{\cdot u}}[E(p_{g \cdot})-E(p_{g|u})] = \sum_g{p_{g \cdot}}[E(p_{\cdot u})-E(p_{u|g})]
$$

De esta manera, el índice $M$ nos indica "cuánta más información" proporciona la distribución general en comparación con la distribución por grupos de una unidad específica (o su contraparte simétrica para grupos). A su vez, esta formulación nos permite definir el límite superior del índice, que será $min({ \log{U}; \log{G}})$.

Por último, el índice $M$ está íntimamente vinculado con el índice de la teoría de la información $H$ [también definido por @Theil1972] mediante la siguiente relación:

$$
  H = \frac{M}{E(p_{g \cdot})}
$$

Si bien el índice $H$ tiene la ventaja de estar acotado entre 0 y 1, @Mora2011 señalan la ambigüedad de su interpretación cuando se realiza una descomposición para múltiples grupos (como en este caso).

**Multi-group normalized exposure Multi-group** ($P^*$)

La fórmula del índice es:

$$
  P^* = \frac{1}{T} \sum_{k=1}^K {\sum_{i=1}^n{\frac{t_i(p_i^k-P^k)^2}{1-P^k}}}
$$
